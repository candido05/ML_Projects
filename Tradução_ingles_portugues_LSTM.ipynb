{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/candido05/ML_Projects/blob/main/Tradu%C3%A7%C3%A3o_ingles_portugues_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPtyqz3fEECO",
        "outputId": "cf52051d-b26e-40f5-ad21-8bb2ba240a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando dataset: nageshsingh/englishportuguese-translation\n",
            "Dataset URL: https://www.kaggle.com/datasets/nageshsingh/englishportuguese-translation\n",
            "License(s): unknown\n",
            "Downloading englishportuguese-translation.zip to /content/englishportuguese-translation\n",
            " 92% 5.00M/5.41M [00:01<00:00, 5.53MB/s]\n",
            "100% 5.41M/5.41M [00:01<00:00, 3.72MB/s]\n",
            "Arquivo englishportuguese-translation.zip descompactado em /content/englishportuguese-translation\n"
          ]
        }
      ],
      "source": [
        "def download_kaggle_dataset(dataset_url, output_path='.', kaggle_username=None, kaggle_key=None):\n",
        "    \"\"\"\n",
        "    Baixa um dataset do Kaggle diretamente no Colab usando o link do repositório, sem interação do usuário.\n",
        "\n",
        "    Parâmetros:\n",
        "    dataset_url (str): URL do dataset no Kaggle (ex: https://www.kaggle.com/datasets/username/dataset-name)\n",
        "    output_path (str): Caminho onde os arquivos serão salvos (padrão é o diretório atual)\n",
        "    kaggle_username (str): Seu username do Kaggle\n",
        "    kaggle_key (str): Sua chave API do Kaggle\n",
        "    \"\"\"\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    try:\n",
        "        dataset_path = '/'.join(dataset_url.split('/')[-2:]).split('?')[0]  # Remove parâmetros extras da URL\n",
        "    except:\n",
        "        raise ValueError(\"URL inválida. Use o formato: https://www.kaggle.com/datasets/username/dataset-name\")\n",
        "\n",
        "    import os\n",
        "    if kaggle_username is None or kaggle_key is None:\n",
        "        raise ValueError(\"Forneça seu kaggle_username e kaggle_key como argumentos.\")\n",
        "\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "        f.write(f'{{\"username\":\"{kaggle_username}\",\"key\":\"{kaggle_key}\"}}')\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    print(f\"Baixando dataset: {dataset_path}\")\n",
        "    !kaggle datasets download -d {dataset_path} -p {output_path}\n",
        "\n",
        "    import zipfile\n",
        "    zip_files = [f for f in os.listdir(output_path) if f.endswith('.zip')]\n",
        "    if zip_files:\n",
        "        for zip_file in zip_files:\n",
        "            with zipfile.ZipFile(f\"{output_path}/{zip_file}\", 'r') as zip_ref:\n",
        "                zip_ref.extractall(output_path)\n",
        "            print(f\"Arquivo {zip_file} descompactado em {output_path}\")\n",
        "            os.remove(f\"{output_path}/{zip_file}\")\n",
        "    else:\n",
        "        print(\"Download concluído, mas nenhum arquivo .zip foi encontrado. Verifique se o dataset foi baixado corretamente.\")\n",
        "\n",
        "kaggle_username = \"\"\n",
        "kaggle_key = \"\"\n",
        "download_kaggle_dataset(\n",
        "    \"https://www.kaggle.com/datasets/nageshsingh/englishportuguese-translation\",\n",
        "    \"/content/englishportuguese-translation\",\n",
        "    kaggle_username=kaggle_username,\n",
        "    kaggle_key=kaggle_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chart-studio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiYAhnPQFDqa",
        "outputId": "501a2e8b-b091-4491-c605-51c850bf2398"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chart-studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from chart-studio) (5.24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from chart-studio) (2.32.3)\n",
            "Collecting retrying>=1.3.3 (from chart-studio)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from chart-studio) (1.17.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->chart-studio) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->chart-studio) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (2025.1.31)\n",
            "Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: retrying, chart-studio\n",
            "Successfully installed chart-studio-1.1.0 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import string\n",
        "\n",
        "import chart_studio.plotly\n",
        "import chart_studio.plotly as py\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "import plotly.graph_objs as go"
      ],
      "metadata": {
        "id": "sTbdRtZiH3wl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/englishportuguese-translation/por.txt'\n",
        "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "lines[5000:5010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o-59DBCID-2",
        "outputId": "d284bc48-78f2-44a1-e332-1467b372e968"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Will it rain?\\tSerá que chove?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #8918600 (CK) & #8930552 (JGEN)',\n",
              " 'Wish me luck.\\tDeseje-me sorte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2254917 (CK) & #872788 (alexmarcelo)',\n",
              " \"Won't you go?\\tVocê não vai?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241051 (CK) & #6212788 (bill)\",\n",
              " 'Write in ink.\\tEscreva à tinta.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3258764 (CM) & #7351595 (alexmarcelo)',\n",
              " 'Write in ink.\\tEscreva a tinta.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3258764 (CM) & #7351606 (alexmarcelo)',\n",
              " 'Write to Tom.\\tEscreva para o Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2240357 (CK) & #5985551 (Ricardo14)',\n",
              " 'Years passed.\\tPassaram os anos.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #282197 (CK) & #977841 (alexmarcelo)',\n",
              " 'Years passed.\\tAnos se passaram.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #282197 (CK) & #2324530 (Matheus)',\n",
              " 'You amuse me.\\tVocê me diverte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #268209 (CM) & #1199960 (alexmarcelo)',\n",
              " 'You are late.\\tVocê está atrasado.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #277403 (CK) & #1275547 (alexmarcelo)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"numero total de registros: \", len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhsquWdrIW8P",
        "outputId": "74249f10-7c4f-41e4-85bc-1ec611f890b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numero total de registros:  168903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exclude = set(string.punctuation)\n",
        "remove_digits = str.maketrans('', '', string.digits)"
      ],
      "metadata": {
        "id": "F8xOaJ-kIyjg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para pré-processamento de sentenças em inglês"
      ],
      "metadata": {
        "id": "8uBiMAC5JvuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_eng_sentence(sent):\n",
        "    sent = sent.lower()\n",
        "    sent = re.sub(\"'\", '', sent)\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
        "    sent = sent.translate(remove_digits)\n",
        "    sent = sent.strip()\n",
        "    sent = re.sub(\" +\", \" \", sent)\n",
        "    sent = '<start> ' + sent + ' <end>'\n",
        "    return sent"
      ],
      "metadata": {
        "id": "_TV8bsVtJDGf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função para pré-processamento de sentenças em português"
      ],
      "metadata": {
        "id": "IvZ5RWJZJxhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_port_sentence(sent):\n",
        "    sent = re.sub(\"'\", '', sent)\n",
        "    sent = ''.join(ch for ch in sent if ch not in exclude)\n",
        "    sent = sent.strip()\n",
        "    sent = re.sub(\" +\", \" \", sent)\n",
        "    sent = '<start> ' + sent + ' <end>'\n",
        "    return sent"
      ],
      "metadata": {
        "id": "1IPS8FEXJtBR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geração de pares de senteças limpas com tokens de início e fim"
      ],
      "metadata": {
        "id": "jHr1nZ27KXzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_pairs = []\n",
        "for line in lines:\n",
        "    sent_pair = []\n",
        "    eng, port = line.rstrip().split('\\t')[:2]\n",
        "    eng = preprocess_eng_sentence(eng)\n",
        "    port = preprocess_port_sentence(port)\n",
        "    sent_pair.extend([eng, port])\n",
        "    sent_pairs.append(sent_pair)\n",
        "\n",
        "sent_pairs[5000:5010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-z0NHLlKXEU",
        "outputId": "b4dd4745-141c-4687-972b-8fd4a2f5f8ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<start> will it rain <end>', '<start> Será que chove <end>'],\n",
              " ['<start> wish me luck <end>', '<start> Desejeme sorte <end>'],\n",
              " ['<start> wont you go <end>', '<start> Você não vai <end>'],\n",
              " ['<start> write in ink <end>', '<start> Escreva à tinta <end>'],\n",
              " ['<start> write in ink <end>', '<start> Escreva a tinta <end>'],\n",
              " ['<start> write to tom <end>', '<start> Escreva para o Tom <end>'],\n",
              " ['<start> years passed <end>', '<start> Passaram os anos <end>'],\n",
              " ['<start> years passed <end>', '<start> Anos se passaram <end>'],\n",
              " ['<start> you amuse me <end>', '<start> Você me diverte <end>'],\n",
              " ['<start> you are late <end>', '<start> Você está atrasado <end>']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação de uma classe para mapear cada palavra de um índice para qualquer vocabulário fornecido e vice-versa."
      ],
      "metadata": {
        "id": "P0K9jz3qLkhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word"
      ],
      "metadata": {
        "id": "BZts_IVRLfKf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "metadata": {
        "id": "kFAozPghMGNx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenização e Padding"
      ],
      "metadata": {
        "id": "GO8dM_DONJ_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(pairs, num_examples):\n",
        "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
        "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
        "\n",
        "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
        "\n",
        "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp, padding='post')\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar, padding='post')\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
      ],
      "metadata": {
        "id": "V4oUVpunNNT1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))"
      ],
      "metadata": {
        "id": "fGJUoJXSPtFj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação dos dados de treinamento e validação"
      ],
      "metadata": {
        "id": "XguP9-5RRNH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.1, random_state=101\n",
        ")\n",
        "\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87zA-O2pQB8g",
        "outputId": "64d03f10-077d-437d-a207-3984be50db87"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152012, 152012, 16891, 16891)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hiperparâmetros e prepara o dataset de treinamento para um modelo de sequência.\n",
        "# BUFFER_SIZE: tamanho do buffer para embaralhar os dados (total de exemplos).\n",
        "# BATCH_SIZE: número de exemplos por lote (64).\n",
        "# N_BATCH: número total de lotes por época.\n",
        "# embedding_dim: dimensão dos vetores de embedding (256).\n",
        "# units: número de unidades em camadas do modelo (1024).\n",
        "# vocab_inp_size e vocab_tar_size: tamanhos dos vocabulários de entrada e alvo.\n",
        "# O dataset é criado a partir dos tensores de entrada e alvo, embaralhado e dividido em lotes de 64 exemplos.\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "pBpztXAPRpJm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construção do modelo Seq2Seq com API funcional"
      ],
      "metadata": {
        "id": "MC1TDBZfTHwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_seq2seq_model(vocab_inp_size, vocab_tar_size, embedding_dim, units):\n",
        "    # Encoder\n",
        "    encoder_inputs = tf.keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "    enc_emb = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)(encoder_inputs)\n",
        "    enc_outputs, state_h, state_c = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)(enc_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = tf.keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "    dec_emb = tf.keras.layers.Embedding(vocab_tar_size, embedding_dim)(decoder_inputs)\n",
        "    dec_lstm = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
        "    dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "    # Atenção\n",
        "    attention = tf.keras.layers.Attention()\n",
        "    context = attention([dec_outputs, enc_outputs])\n",
        "    dec_outputs = tf.keras.layers.Concatenate(axis=-1)([dec_outputs, context])\n",
        "\n",
        "    # Camada densa\n",
        "    outputs = tf.keras.layers.Dense(vocab_tar_size, activation='softmax')(dec_outputs)\n",
        "\n",
        "    # Modelo\n",
        "    model = tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8fZF4jr4TGPs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_seq2seq_model(vocab_inp_size, vocab_tar_size, embedding_dim, units)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "C_wJq5-JUfXv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "dAWTnrLR5ais",
        "outputId": "0b26dc3b-59d4-450c-ffce-7b1f3f5f7177"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,239,168\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m6,423,296\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m),   │      \u001b[38;5;34m5,246,976\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │                │                        │\n",
              "│                           │ \u001b[38;5;34m1024\u001b[0m)]                 │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m),   │      \u001b[38;5;34m5,246,976\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m] │\n",
              "│                           │ \u001b[38;5;34m1024\u001b[0m)]                 │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (\u001b[38;5;33mAttention\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25091\u001b[0m)    │     \u001b[38;5;34m51,411,459\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,239,168</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,423,296</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>),   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]                 │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>),   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>] │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]                 │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25091</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,411,459</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m71,567,875\u001b[0m (273.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,567,875</span> (273.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,567,875\u001b[0m (273.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,567,875</span> (273.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação dos dados do decoder (teacher forcing)"
      ],
      "metadata": {
        "id": "Aqbp-OGHUlu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_data = np.zeros_like(target_tensor_train)\n",
        "decoder_input_data[:, :-1] = target_tensor_train[:, 1:]  # Desloca para a esquerda\n",
        "decoder_input_data[:, -1] = 0  # Adiciona padding no final"
      ],
      "metadata": {
        "id": "_mp5V-URsDb2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento do modelo"
      ],
      "metadata": {
        "id": "OM0S6heisBeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Habilite precisão mista\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "mgY9qZJscNb8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduza o batch_size e o número de unidades\n",
        "BATCH_SIZE = 32\n",
        "units = 512\n",
        "\n",
        "# Treinamento do modelo\n",
        "EPOCHS = 3\n",
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        [input_tensor_train, decoder_input_data],\n",
        "        target_tensor_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[tf.keras.callbacks.ModelCheckpoint(\n",
        "            '/content/checkpoints/model_{epoch:02d}.weights.h5',\n",
        "            save_weights_only=True,\n",
        "            save_best_only=True,\n",
        "            monitor='val_loss'\n",
        "        )]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMrBvrm00D1",
        "outputId": "9b9840ce-ed1a-44a5-c187-2a2ea3740650"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3801/3801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1380s\u001b[0m 361ms/step - accuracy: 0.9075 - loss: 0.6729 - val_accuracy: 0.9850 - val_loss: 0.1169\n",
            "Epoch 2/3\n",
            "\u001b[1m3801/3801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1405s\u001b[0m 363ms/step - accuracy: 0.9895 - loss: 0.0718 - val_accuracy: 0.9918 - val_loss: 0.0675\n",
            "Epoch 3/3\n",
            "\u001b[1m3801/3801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1414s\u001b[0m 366ms/step - accuracy: 0.9965 - loss: 0.0180 - val_accuracy: 0.9930 - val_loss: 0.0599\n"
          ]
        }
      ]
    }
  ]
}